[task parameters]
    # run multiple instances
    m = 0..3
    n = 4, 16
[scheduling] # Define the tasks and when they should run
  [[graph]]
    R1 = """ # run this graph once
    run<m, n>? => analyse
    """
[runtime] # Define what each task should run
  [[run<m, n>]] # Default settings inherited by all tasks
    platform = mahuika-slurm # Run "cylc conf" to see platforms. 
    execution retry delays = 1*PT10S # retry
    script = """
      module purge
      module load intel
      module load R
      cd $TOP_DIR
      work_dir="${TOP_DIR}/workingFolder-${SLURM_JOB_ID}"
      mkdir $work_dir
      # set the number of cores and workers
      cat scaling.R | perl -ne "s/ncores\s*\<\-\s*\d+/ncores <- $SLURM_TASK_PARAM_n + 1/;print;" > scaling-${SLURM_JOB_ID}.R
      diff scaling.R scaling-${SLURM_JOB_ID}.R
      # run
      export I_MPI_SPAWN=on
      export I_MPI_DEBUG=10
      export FI_LOG_LEVEL=debug
      export I_MPI_PIN_RESPECT_CPUSET=0
      export FI_PROVIDER=mlx # faster
      mpirun -bootstrap slurm Rscript scaling-${SLURM_JOB_ID}.R $work_dir
      rm -rf $work_dir
    """
    [[[directives]]] # Default SLURM options for the tasks below
       --account = nesi99999 # CHANGE
       --hint = nomultithread
       --mem = 40GB
       --time = 06:00:00
       --ntasks = 21
       --cpus-per-task = 1
    [[[environment]]]
      TOP_DIR="/nesi/nobackup/pletzera/rswat2/" # CHANGE
  [[run-milan-slurm<m>]]
    [[[directives]]]
      --partition = milan
  [[run-milan-slurm-mlx<m>]]
    [[[directives]]]
      --partition = milan
    [[[environment]]]
      FI_PROVIDER = mlx
  [[analyse]]
    platform = localhost
    script = """
        # create plot
        module load Python
        cd /nesi/nobackup/pletzera/cylc-rswat/rswat-medium
        python analyse.py
    """

