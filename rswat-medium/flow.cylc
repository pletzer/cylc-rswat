[task parameters]
    # run multiple instances
    m = 0..2
[scheduling] # Define the tasks and when they should run
  [[graph]]

    R1 = """ # run this graph once
    run-milan-slurm<m>? & run-milan-slurm-mlx<m>? => analyse
    """
[runtime] # Define what each task should run
  [[root]] # Default settings inherited by all tasks
    platform = mahuika-slurm # Run "cylc conf" to see platforms. 
    execution retry delays = 1*PT10S # retry
    script = """
      module purge
      module load intel
      module load R
      cd $TOP_DIR
      work_dir="${TOP_DIR}/workingFolder-${SLURM_JOB_ID}"
      mkdir $work_dir
      export I_MPI_SPAWN=on
      export I_MPI_DEBUG=10
      export FI_LOG_LEVEL=debug
      export I_MPI_PIN_RESPECT_CPUSET=0
      mpirun -bootstrap slurm Rscript medium.R $work_dir
      rm -rf $work_dir
    """
    [[[directives]]] # Default SLURM options for the tasks below
       --account = nesi99999 # CHANGE
       --hint = nomultithread
       --mem = 40GB
       --time = 01:00:00
       --nodes = 2
       --ntasks = 21
       --cpus-per-task = 1
    [[[environment]]]
      TOP_DIR="/nesi/nobackup/pletzera/rswat2/" # CHANGE
  [[run-milan-slurm<m>]]
    [[[directives]]]
      --partition = milan
  [[run-milan-slurm-mlx<m>]]
    [[[directives]]]
      --partition = milan
    [[[environment]]]
      FI_PROVIDER = mlx
  [[analyse]]
    platform = localhost
    script = """
        # create plot
        module load Python
        cd /nesi/nobackup/pletzera/cylc-rswat/rswat-medium
        python analyse.py
    """

